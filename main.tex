% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016
\documentclass[runningheads]{llncs}
\usepackage{eso-pic}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{amsfonts,bm}
%\usepackage{amsthm}
\usepackage[ruled,lined,boxed]{algorithm2e}
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

% Mathematical Definitions
\newtheorem{assumption}{Assumption}

\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\PsiTens}{\mathbf{\Psi}}
\def\vomega{{\bm{\omega}}}
\def\va{{\bm{a}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV16SubNumber{***}  % Insert your submission number here

\title{Why Vision--Language Explanations Hallucinate: A Geometric Analysis and Orthogonal Remedy}

\titlerunning{ECCV-16 submission ID \ECCV16SubNumber}

\authorrunning{ECCV-16 submission ID \ECCV16SubNumber}

\author{Anonymous ECCV submission}
\institute{Paper ID \ECCV16SubNumber}


\maketitle

\begin{abstract}
As Vision--Language Models are increasingly deployed in safety-critical applications, the trustworthiness of their explanations becomes crucial. Explainable AI (XAI) methods for Vision-Language Models often suffer from \textit{semantic hallucination}, where attribution maps highlight prominent image regions even when prompted with incorrect text descriptions (e.g., highlighting a dog when prompted ``cat''). Although this problem is widespread, a formal mathematical analysis of XAI methods and CLIP embeddings is largely missing in the literature. We demonstrate that this phenomenon is not specific to a single architecture but is a fundamental consequence of Linear Semantic Leakage in high-dimensional embedding spaces. We propose a unified theoretical framework, Linear Semantic Attribution (LSA), which generalizes across discriminative methods. We introduce OSP, a geometric intervention that utilizes the residual property of OMP to disentangle unique semantic signals from shared concepts. We prove theoretically and demonstrate empirically that OSP minimizes hallucination by orthogonalizing the query vector against distractor concepts, rendering the attribution model blind to shared features while preserving fidelity for correct prompts.
\keywords{Vision-Language Models, Explainable AI, Hallucination, Orthogonal Matching Pursuit, CLIP, SigLIP, Diffusion Models}
\end{abstract}

\section{Introduction}

Vision--Language Models (VLMs) and multimodal diffusion models have become central to modern computer vision. Well-known examples include CLIP~\cite{radford2021learning} and SigLIP~\cite{zhai2023sigmoid} for image--text understanding, as well as generative architectures such as Stable Diffusion~\cite{rombach2022high}, Flux~\cite{esser2024scaling}, and PixArt-$\alpha$~\cite{chen2024pixart}. These systems have enabled significant progress in areas such as autonomous driving~\cite{zhou2024vision}, medical image synthesis~\cite{kazerouni2023diffusion}, and zero-shot robotics~\cite{nasiriany2024pivot}. However, as they move from research prototypes to real-world deployment, a critical challenge emerges: they are difficult to interpret. Their internal representations function largely as ``black boxes,'' undermining trust, especially in safety-critical environments.

To better understand these models, researchers commonly apply post-hoc explainability methods originally developed for standard deep neural networks. These methods produce saliency maps or attribution masks that highlight important regions in an image. Yet when applied to multimodal models, a new and severe problem appears: \textit{semantic hallucination}. Saliency methods often highlight objects that are completely absent from the image, for instance, highlighting a dog when prompted with ``cat.'' This undermines the trustworthiness of Explainable AI (XAI) and, more broadly, reduces the possibility of effectively debugging AI models and makes AI less safe.

Hallucination is commonly defined as generated content that is incorrect or not grounded in the input. In this work, we show that hallucination does not only affect model \emph{outputs}: it also affects \emph{explanations}. We define \textbf{explanation hallucination} as a situation where a saliency map highlights image regions that are not truly related to the given text prompt. In other words, the explanation appears convincing but is not correctly grounded in the text: the model may seem correct, but it is ``right for the wrong reasons.''

We argue that this issue stems from the geometry of the multimodal embedding space. Image and text features are represented in a shared high-dimensional space. Because these embeddings are not orthogonal, semantically related but distinct concepts share common directional components. When saliency maps are computed using these embeddings, these shared components cause the method to highlight irrelevant regions: producing hallucinated explanations.

In this paper, we introduce \textbf{Orthogonal Semantic Pursuit (OSP)}, a geometric intervention designed to reduce hallucinations in saliency-based explanations. OSP is grounded in a simple geometric idea: before computing the saliency map, we remove the shared semantic components in the text embedding that cause hallucination. To achieve this, we decompose the text embedding into interpretable semantic components using a dictionary learning framework based on Orthogonal Matching Pursuit (OMP). This sparse decomposition allows us to isolate and remove the parts of the embedding that introduce semantic leakage. We then compute the saliency map using a refined, orthogonalized representation of the text query. Importantly, OSP is \textit{plug-and-play}: it can be applied to a wide range of Vision--Language models and saliency methods without retraining the underlying model.

\subsection{Contributions}
\begin{enumerate}
    \item \textbf{Geometric Analysis of Hallucination:} We provide a formal mathematical framework that bridges the gap between grounding failures and hallucinatory outputs, characterizing hallucination as a function of the latent misalignment (i.e.\ non-orthogonality) between semantic embedding directions and offering a geometric interpretation of how grounding errors propagate into nonsensical attributions.
    \item \textbf{A Universal XAI Technique:} OSP is compatible with multiple architectures, including CLIP, SigLIP, and diffusion-based models, and can be integrated with different saliency methods as a plug-and-play module.
    \item \textbf{Extensive Validation:} We evaluate OSP across 3 foundational models and 5 state-of-the-art XAI methods, demonstrating consistent improvements.
    \item \textbf{Practical Use Cases and User Study:} We demonstrate improved interpretability reliability, backed by empirical results and a user study.
\end{enumerate}

\paragraph{Scope.} Our method targets object attribution hallucination, not visual question answering (VQA).

\section{Related Work}

\subsection{Vision-Language Encoder Attribution Methods}
Attribution methods and saliency maps aim to demystify the ``black box'' nature of Vision-Language Models (VLMs) by grounding textual concepts in visual regions. Discriminative approaches adapt gradient-based strategies to multimodal architectures. For instance, GradCAM \cite{selvaraju2017grad} has been successfully applied to CLIP to identify visual features that maximize similarity with specific text queries. For Vision Transformers (ViTs), methods like LeGrad \cite{legrad2025} provide feature formation sensitivity by tracing gradients to internal patch embeddings, while CheferCAM \cite{chefer2021} directly propagates relevance scores using attention maps and their gradients. Generative models, such as Stable Diffusion, rely on methods like DAAM \cite{tang-etal-2023-daam}, which aggregates cross-attention maps to spatially localize the influence of prompt words. Recent works such as TextSpan \cite{textspan2024} evaluate concept attribution by decomposing image representations into text-based constituents. However, unlike these existing explainability approaches that accept raw entangled query embeddings and refine spatial attribution post-hoc or within bottlenecks, our method performs a structured geometric intervention directly in the continuous representation space prior to attribution to proactively resolve spatial leakage caused by shared semantic similarities.

\subsection{Hallucination in Vision-Language Models}
Despite their remarkable zero-shot capabilities, VLMs and Large Vision-Language Models (LVLMs) frequently suffer from object hallucination: the generation of outputs or heatmaps that incorrectly assert the presence of objects inconsistent with the image. The severity of this issue has motivated the creation of targeted evaluation benchmarks. The CHAIR metric \cite{rohrbach2018object} specifically quantifies object hallucinations in image captioning generation. More recently, the POPE benchmark \cite{li2023evaluating} was introduced to robustly evaluate object hallucination in LVLMs through systematic yes-or-no polling questions, revealing that even state-of-the-art models struggle with object co-occurrence biases and visual distractors. To mitigate concept hallucination specifically within CLIP-based Concept Bottleneck Models (CBMs), Kazmierczak et al. \cite{kazmierczak2025enhancing} introduce CHILI, a technique that disentangles image embeddings to localize concept pixels and inhibit false predictions. While these diagnostic and inhibitory tools highlight the prevalence of hallucination as an empirical phenomenon to be benchmarked or mitigated via targeted disentanglement, our work uniquely establishes a formal, mathematical link connecting the root cause of these errors backward into the linear geometry of the multimodal representation space itself.

\subsection{Representation Geometry of Vision-Language Encoders}
Emerging research characterizes the high-dimensional latent spaces of networks like CLIP not as uniform structures, but as complex geometric manifolds. The Linear Representation Hypothesis \cite{zou2023representation} suggests that high-level concepts and hierarchical structures are encoded as linear directions. Furthermore, the Platonic Representation Hypothesis \cite{huh2024platonic} posits that diverse architectures naturally converge toward a shared geometric model of reality. Within contrastive models like CLIP, geometric phenomena such as the ``modality gap'' \cite{liang2022mind} reveal that image and text embeddings reside on separated, offset hyperspheres, severely restricting the angular portion of the space utilized by the model. To disentangle these compressed spaces, Sparse Autoencoders (SAEs) \cite{bricken2023towards} have recently been employed to extract monosemantic, interpretable directions from dense activations. While prior works focus on analyzing structures like the modality gap or sparse features, OSP uniquely utilizes these geometric insights as an active, closed-loop mitigation tool strictly dynamically orthogonalizing directions at inference time to mechanically correct flawed model behavior and concept bleeding.


\section{The Geometry of Hallucination}

\subsection{Background on Saliency Map Methods}
Before introducing our unified framework, we summarize the prominent attribution methods in the literature, spanning both discriminative (CLIP-based) and generative (Diffusion-based) architectures.

\subsubsection{Discriminative Methods (ViT/CLIP)}
\textbf{GradCAM \cite{selvaraju2017grad}:} Originally designed for CNNs, GradCAM computes saliency maps by weighting feature maps $\mathbf{A}^k$ with the gradients of the target class score with respect to the activations ($w_k$). In the context of CLIP, the score $S_c$ is defined by the dot product between the global image embedding $\va^{\text{img}}$ and the text embedding $\va^{\text{txt}}_c$.

\textbf{LeGrad \cite{legrad2025}:} Designed for Vision Transformers (ViT), LeGrad computes the backward gradient of the output class token $\va^{\text{img}}$ with respect to the internal patch embeddings $\mathbf{A}^b_j$. This provides a sensitivity map indicating which patches contribute most to the final classification.

\textbf{CheferCAM \cite{chefer2021}:} A generic attention explainability method that computes relevance maps for Transformers by propagating relevance scores from the output to the input, weighted by the gradients $\nabla_{\mathbf{A}^l} S_c$ and attention maps $\mathbf{A}^l$ at each layer $l$.

\textbf{AttentionCAM \cite{chefer2020}:} A visualization method that uses the last layer attention maps $\mathbf{A}^L$ scaled by their gradients $\nabla_{\mathbf{A}^L} S_c$ to identify salient regions.

\subsubsection{Generative Methods (Diffusion)}
\textbf{DAAM \cite{tang-etal-2023-daam}:} Diffusion Attentive Attribution Maps (DAAM) are designed for Stable Diffusion (specifically tested on the Stable Diffusion 2 backbone). DAAM aggregates cross-attention maps $\mathbf{A}_{\text{cross}}^{(t,h)}$ across timesteps $t \in T$ and attention heads $h \in \mathcal{H}$ to spatially localize the influence of specific words in the prompt.

We propose a unifying formulation for Vision-Language XAI. Despite architectural differences, state-of-the-art methods generate saliency maps via a linear contraction between visual features and semantic queries.

\begin{definition}[Visual Projection Tensor]
Let $\vx$ be an image processed by a backbone $\mathcal{V}$ (ViT or Diffusion UNet). We define the \textbf{Visual Projection Tensor} $\PsiTens(\vx) \in \R^{H \times W \times d}$, representing the local visual features aligned with the semantic space.
\end{definition}

\begin{definition}[Linear Semantic Attribution]
A generalized attribution map $\mathbf{V}_c(\vx) \in \R^{H \times W}$ for a text query $\va^{\text{txt}}_c \in \R^d$ is generated by the tensor contraction:
\begin{equation}
    \mathbf{V}_c(\vx) = f(\PsiTens(\vx) \times_d \va^{\text{txt}}_c)
\end{equation}
where $\times_d$ denotes the dot product along the embedding dimension.
\end{definition}

\textbf{Instantiations of the Framework.} This LSA framework generalizes across the methods reviewed:
\begin{itemize}
    \item \textbf{LeGrad (CLIP/SigLIP):} $\PsiTens(\vx) = \nabla_{\mathbf{A}} \va^{\text{img}}$. The gradient of the image embedding $\va^{\text{img}}$ with respect to the internal patch activations $\mathbf{A}^b_j$ is treated as a linear operator on the text projection.
    \item \textbf{GradCAM (CLIP/SigLIP):} Operates in \emph{feature space}. The gradient of the similarity score $S_c = \inner{\va^{\text{img}}}{\va^{\text{txt}}_c}$ with respect to each feature channel $\mathbf{A}^k$ at a chosen layer yields a per-channel scalar importance weight: $\PsiTens(\vx) = \sum_k w_k \mathbf{A}^k$, where $w_k = \frac{\partial S_c}{\partial \mathbf{A}^k}$.
    \item \textbf{CheferCAM \cite{chefer2021} (CLIP/SigLIP):} Operates in \emph{attention space} with fine-grained, per-element gradient weighting. The gradient of $S_c$ with respect to the attention map at each layer is multiplied element-wise with the attention map, clamped by ReLU to retain only positively contributing attention connections, and averaged across heads. The per-layer maps are then aggregated across layers: $\PsiTens(\vx) = \sum_{l=l_0}^{L} E_h(\nabla_{\mathbf{A}^l} S_c \odot \mathbf{A}^l)^{+}$. A computationally efficient last-layer variant using only $\mathbf{A}^L$ can also be adopted.
    \item \textbf{AttentionCAM \cite{chefer2020} (CLIP/SigLIP):} A single-layer variant that also operates in \emph{attention space}. The gradient of the similarity score $S_c$ with respect to the last attention map $\mathbf{A}^L$ is multiplied element-wise with the attention map, clamped by ReLU, and averaged across heads: $\PsiTens(\vx) = E_h(\nabla_{\mathbf{A}^L} S_c \odot \mathbf{A}^L)^{+}$. An alternative simplification collapses the gradient signal into a single scalar weight: $\PsiTens(\vx) = \bar{w} \cdot \mathbf{A}^L, \; \bar{w} = \text{mean}(\nabla_{\mathbf{A}^L} S_c)$.
    \item \textbf{DAAM (Stable Diffusion):} Cross-attention maps $\mathbf{A}_{\text{cross}}^{(t,h)}$ across timesteps $t$ and attention heads $h$. $\PsiTens(\vx) = \frac{1}{T} \sum_{t=1}^{T} \frac{1}{|\mathcal{H}|} \sum_{h \in \mathcal{H}} \mathbf{A}_{\text{cross}}^{(t,h)}$.
\end{itemize}

\noindent Formally, the saliency for concept $c$ given input $\mathbf{x}$ is:
\begin{equation}
\begin{aligned}\label{eq:method_cases}
\mathbf{V}_c(\mathbf{x}) =
    ~~ &\begin{cases}
        \mathbf{w}_d^c \, \mathbf{A}_d(\mathbf{x})\text{ with } \mathbf{w}_d^c = \va^{\text{txt}}_c , & \text{(\textbf{CAM})}, \\
        \mathbf{w}_d^c \, \mathbf{A}_d(\mathbf{x})\text{ with } \mathbf{w}_d^c = \frac{\partial}{\partial \va^{\text{txt}}_c}\langle \va^{\text{txt}}_c, \va^{\text{img}}_d \rangle, & \text{(\textbf{GradCAM}~\cite{selvaraju2017grad})}, \\
        \sum_{j=1}^d \mathbf{w}_j^c \, \mathbf{A}^b_j(\mathbf{x}),\text{ with } \mathbf{w}_j^c = 1/(dh) , & \text{(\textbf{LeGrad}~\cite{legrad2025})}, \\
        \sum_{l=l_0}^{L} E_h\bigl(\nabla_{\mathbf{A}^l} S_c \odot \mathbf{A}^l(\mathbf{x})\bigr)^{+}, & \text{(\textbf{CheferCAM}~\cite{chefer2021})}, \\
        E_h\bigl(\nabla_{\mathbf{A}^L} S_c \odot \mathbf{A}^L(\mathbf{x})\bigr)^{+}, & \text{(\textbf{AttentionCAM}~\cite{chefer2020})}, \\
        \frac{1}{T} \sum_{t=1}^{T} \frac{1}{|\mathcal{H}|} \sum_{h \in \mathcal{H}} \mathbf{A}_{\text{cross}}^{(t,h)}(\mathbf{x}, c), & \text{(\textbf{DAAM}~\cite{tang-etal-2023-daam})},
    \end{cases}
\end{aligned}
\end{equation}
where:
\begin{itemize}
    \item $c$ denotes the target class or concept,
    \item $\va^{\text{txt}}_c \in \R^d$ is the text embedding for concept $c$,
    \item $\va^{\text{img}}$ is the image embedding (e.g., the class token in ViT or an averaged feature token),
    \item $\mathbf{A}_d(\vx)$ represents the forward activation maps for spatial features,
    \item $\mathbf{A}^b_j(\vx)$ refers to the backward gradients of the activations,
    \item $S_c = \langle \va^{\text{txt}}_c, \va^{\text{img}} \rangle$ denotes the cosine similarity score for concept $c$,
    \item $E_h$ denotes the mean aggregation across attention heads,
    \item $\mathbf{A}^l(\vx)$ represents the attention map at layer $l$,
    \item $\mathbf{A}_{\text{cross}}^{(t,h)}(\vx, c)$ refers to the cross-attention map for token $c$ at diffusion timestep $t$ and head $h$,
    \item $(\cdot)^{+}$ denotes the ReLU activation function,
    \item $\odot$ denotes the element-wise Hadamard product.
\end{itemize}

\textbf{From Linear Attribution to Semantic Leakage.}
The key observation is that the LSA equation $\mathbf{V}_c(\vx) = f(\PsiTens(\vx) \times_d \va^{\text{txt}}_c)$ is \emph{linear in the query embedding} $\va^{\text{txt}}_c$. This linearity has a critical consequence: if a query vector $\va^{\text{txt}}_c$ can be decomposed into a sum of components, the resulting saliency map decomposes into a corresponding sum of maps—one for each component.

In a contrastive embedding space such as CLIP or SigLIP, semantically related concepts are mapped to nearby directions on the unit hypersphere. For example, the text embeddings for ``cat'' and ``dog'' are far from orthogonal because both share \textit{animal} semantics that the encoder captures as overlapping directions. We now show that this geometric overlap, combined with the linearity of LSA, is the precise mechanism behind attribution hallucination.

\textbf{Notation.}
Let $\va^{\text{txt}}_A, \va^{\text{txt}}_B \in \R^d$ denote the text embeddings of two classes $A$ and~$B$, and let $\theta_{AB} := \angle(\va^{\text{txt}}_A, \va^{\text{txt}}_B)$ be the angle between them. Because contrastive models normalize embeddings to the unit hypersphere, the inner product reduces to $\inner{\va^{\text{txt}}_A}{\va^{\text{txt}}_B} = \cos\theta_{AB}$. We denote by $\va^{\text{txt}}_{B\perp A}$ the component of $\va^{\text{txt}}_B$ that is orthogonal to $\va^{\text{txt}}_A$, i.e., the part of $\va^{\text{txt}}_B$ that cannot be explained by $\va^{\text{txt}}_A$.

\subsection{Linear Semantic Leakage}

\begin{assumption}[Unit-Norm Embeddings]
    All text embeddings are $\ell_2$-normalized, i.e., $\norm{\va} = 1$ for every embedding $\va$. This is standard for contrastive models such as CLIP and SigLIP, which project embeddings onto the unit hypersphere.
\end{assumption}

\begin{theorem}[Linear Leakage]
    Let image $\vx$ contain an object of class $A$ and let all embeddings satisfy Assumption~1. Let $\va^{\text{txt}}_B$ be the prompt for a distractor class $B$. If $\inner{\va^{\text{txt}}_A}{\va^{\text{txt}}_B} \neq 0$, the attribution map $\mathbf{V}_B(\vx)$ contains a ``Ghost Signal'' proportional to the saliency of the target $A$.
\end{theorem}

\begin{proof}
    Since $\norm{\va^{\text{txt}}_A}=1$ (Assumption~1), the orthogonal projection of $\va^{\text{txt}}_B$ onto $\va^{\text{txt}}_A$ gives the decomposition:
    $$ \va^{\text{txt}}_B = \inner{\va^{\text{txt}}_B}{\va^{\text{txt}}_A}\, \va^{\text{txt}}_A + \va^{\text{txt}}_{B\perp A} = \cos(\theta_{AB})\, \va^{\text{txt}}_A + \va^{\text{txt}}_{B\perp A} $$
    where $\va^{\text{txt}}_{B\perp A} \perp \va^{\text{txt}}_A$ is the component of $\va^{\text{txt}}_B$ orthogonal to $\va^{\text{txt}}_A$, and the second equality uses $\inner{\va^{\text{txt}}_B}{\va^{\text{txt}}_A} = \norm{\va^{\text{txt}}_B}\norm{\va^{\text{txt}}_A}\cos\theta_{AB} = \cos\theta_{AB}$ since both are unit-norm. Substituting into the LSA equation:
    $$ \mathbf{V}_B(\vx) = \PsiTens(\vx) \times_d (\cos(\theta_{AB})\, \va^{\text{txt}}_A + \va^{\text{txt}}_{B\perp A}) $$
    By linearity of the tensor contraction:
    $$ \mathbf{V}_B(\vx) = \cos(\theta_{AB})\underbrace{(\PsiTens(\vx) \times_d \va^{\text{txt}}_A)}_{\text{Target Saliency}} + \underbrace{(\PsiTens(\vx) \times_d \va^{\text{txt}}_{B\perp A})}_{\text{Residual Noise}} $$
    
    \textbf{Analysis:} The term $(\PsiTens(\vx) \times_d \va^{\text{txt}}_A)$ is the correct heatmap for the object present in the image (e.g., the Dog). The scalar $\cos(\theta_{AB})$ is the semantic overlap. Therefore, querying for ``Cat'' retrieves the ``Dog'' heatmap, scaled by their similarity.
\end{proof}

\textbf{Extension to Multiple Concepts.} The single-distractor result generalizes naturally. Let $\va^{\text{txt}}_A$ be the target prompt embedding and let the distractor context be a set of concepts $\mathbf{D} = \{D_1, \dots, D_n\}$ with unit-norm embeddings $\va^{\text{txt}}_{D_i}$. Any query $\va^{\text{txt}}_B$ in the span of $\{\va^{\text{txt}}_A\} \cup \{\va^{\text{txt}}_{D_1},\dots,\va^{\text{txt}}_{D_n}\}$ can be decomposed as:
$$ \va^{\text{txt}}_B = \underbrace{\inner{\va^{\text{txt}}_B}{\va^{\text{txt}}_A}\, \va^{\text{txt}}_A}_{\text{leakage from target}} + \sum_{i=1}^{n} \beta_i\, \va^{\text{txt}}_{D_i} + \va^{\text{txt}}_{B\perp} $$
where $\beta_i$ are the coordinates in the distractor subspace and $\va^{\text{txt}}_{B\perp}$ is the component orthogonal to all of $\{\va^{\text{txt}}_A, \va^{\text{txt}}_{D_1},\dots,\va^{\text{txt}}_{D_n}\}$. Substituting into the LSA equation and applying linearity:
$$ \mathbf{V}_B(\vx) = \inner{\va^{\text{txt}}_B}{\va^{\text{txt}}_A}\,(\PsiTens(\vx) \times_d \va^{\text{txt}}_A) + \sum_{i=1}^{n} \beta_i\,(\PsiTens(\vx) \times_d \va^{\text{txt}}_{D_i}) + (\PsiTens(\vx) \times_d \va^{\text{txt}}_{B\perp}) $$
The first term is the hallucinated ghost signal from the target, the summation captures cross-talk among distractors, and the last term is noise orthogonal to all known concepts.

\subsection{Theoretical Guarantee}

Because the hallucination on distractor set $\mathbf{D} = \{D_1, \dots, D_k\}$ with embeddings $\va^{\text{txt}}_{D_i}$ is linearly proportional to the inner products $\inner{\va^{\text{txt}}_{\text{target}}}{\va^{\text{txt}}_{D_i}}$, driving these inner products to zero mathematically guarantees the elimination of the ghost signals.

\begin{lemma}[Orthogonality of OMP Residuals]
    Let $\mathbf{r}$ be the residual obtained by applying Orthogonal Matching Pursuit to $\va^{\text{txt}}_{\text{target}}$ with dictionary $\{\va^{\text{txt}}_{D_1},\dots,\va^{\text{txt}}_{D_k}\}$. Then $\mathbf{r}$ is strictly orthogonal to every selected atom: $\inner{\mathbf{r}}{\va^{\text{txt}}_{D_i}} = 0$ for all $i \in \{1,\dots,k\}$.
\end{lemma}

By combining Lemma~1 and the Linear Leakage Theorem, we guarantee that substituting $\va^{\text{txt}}_{\text{target}}$ with the OMP residual $\mathbf{r}$ eliminates the ghost signals emanating from all distractor concepts in $\mathbf{D}$.


\section{Methodology}

To resolve Linear Leakage across both single and multiple concepts, we apply OSP to the text embeddings \textit{before} they enter the attribution process.

\subsection{OSP: Orthogonal Query Purification}

The goal is to strip away the components of the target embedding that can be explained by known distractors, retaining only the semantically unique signal.

\textbf{Inputs.} Target concept $C_{\text{target}}$ with unit-norm embedding $\va^{\text{txt}}_{\text{target}} \in \R^d$; distractor/context set $\mathbf{D}=\{C_1,\dots,C_k\}$ with unit-norm embeddings $\va^{\text{txt}}_{D_i}$.

\textbf{Output.} An orthogonalized query $\mathbf{r}$ that preserves the unique part of $C_{\text{target}}$ and satisfies $\inner{\mathbf{r}}{\va^{\text{txt}}_{D_i}} = 0$ for all $i$.

\textbf{Intuition.} Iteratively explain as much of $\va^{\text{txt}}_{\text{target}}$ as possible using distractors; use the leftover (the residual) as the cleaned query.

\begin{algorithm}[t]
\caption{OSP: Orthogonal ``Semantic'' Pursuit}
\label{alg:semomp}
\KwIn{Target embedding $\va^{\text{txt}}_{\text{target}} \in \R^d$; distractor dictionary $\mathbf{D} = [\va^{\text{txt}}_{D_1}, \dots, \va^{\text{txt}}_{D_k}] \in \R^{d \times k}$; number of iterations $T \leq k$}
\KwOut{Orthogonalized query $\mathbf{r} \in \R^d$}
$\mathbf{r}^{(0)} \leftarrow \va^{\text{txt}}_{\text{target}}$\;
$\Lambda^{(0)} \leftarrow \emptyset$\;
\For{$t = 1, \dots, T$}{
    $j^\star \leftarrow \arg\max_{j \notin \Lambda^{(t-1)}} |\inner{\mathbf{r}^{(t-1)}}{\va^{\text{txt}}_{D_j}}|$ \tcp*{Greedy atom selection}
    $\Lambda^{(t)} \leftarrow \Lambda^{(t-1)} \cup \{j^\star\}$\;
    $\mathbf{D}_{\Lambda} \leftarrow [\va^{\text{txt}}_{D_j}]_{j \in \Lambda^{(t)}}$ \tcp*{Selected atoms}
    $\mathbf{r}^{(t)} \leftarrow \va^{\text{txt}}_{\text{target}} - \mathbf{D}_{\Lambda}(\mathbf{D}_{\Lambda}^\top \mathbf{D}_{\Lambda})^{-1} \mathbf{D}_{\Lambda}^\top \va^{\text{txt}}_{\text{target}}$ \tcp*{Orthogonal projection}
}
$\mathbf{r} \leftarrow \mathbf{r}^{(T)} / \norm{\mathbf{r}^{(T)}}$ \tcp*{Re-normalize to unit sphere}
\Return{$\mathbf{r}$}
\end{algorithm}

Algorithm~\ref{alg:semomp} proceeds in $T$ iterations. At each step it greedily selects the distractor atom with the highest absolute correlation to the current residual (line~4), adds it to the active set $\Lambda$ (line~5), and recomputes the residual by projecting $\va^{\text{txt}}_{\text{target}}$ onto the orthogonal complement of the span of the selected atoms (line~7). The final residual is re-normalized to lie on the unit hypersphere. By construction, the output satisfies the conditions of Lemma~1, guaranteeing the elimination of ghost signals. Setting $T = k$ orthogonalizes against all distractors; smaller $T$ trades completeness for fidelity to the original query direction.

\textbf{Generalization Across Discriminative Methods.}
The core principle of OSP, removing semantic leakage from the query vector, applies directly to all four discriminative attribution methods, since each produces saliency through a linear contraction that depends on $\va^{\text{txt}}_c$.
\begin{itemize}
    \item \textbf{LeGrad:} By replacing $\va^{\text{txt}}_c$ with the orthogonal residual $\mathbf{r}$, we ensure that $\PsiTens(\vx) \times_d \mathbf{r}$ yields zero response for image regions containing only distractor features.
    \item \textbf{GradCAM:} If $\va^{\text{txt}}_c$ is orthogonalized against distractors, the similarity score $S = \inner{\va^{\text{img}}}{\mathbf{r}}$ becomes invariant to distractor features, zeroing out the corresponding gradients $w_k$.
    \item \textbf{CheferCAM:} Replacing $\va^{\text{txt}}_c$ with $\mathbf{r}$ eliminates the gradient signal for distractor-aligned attention patterns at every layer.
    \item \textbf{AttentionCAM:} The similarity score $S_c$ no longer responds to shared components for distractor features when $\va^{\text{txt}}_c$ is orthogonalized.
\end{itemize}

\textbf{Key-Space Variant for DAAM.}
Diffusion-based attribution via DAAM operates differently from discriminative methods: saliency arises from cross-attention maps inside the U-Net rather than from a direct dot product with a global text embedding. Consequently, we cannot simply replace a single query vector. Instead, we apply OSP in the \textit{key space} of the cross-attention layers. Concretely, for each cross-attention layer $l$ and each attention head $h$, let $\mathbf{K}_{\text{target}}^{(l,h)}$ be the key vector corresponding to the target token and $\mathbf{K}_{D_i}^{(l,h)}$ the key vectors of the distractor tokens. We run Algorithm~\ref{alg:semomp} independently per layer and head, replacing $\va^{\text{txt}}_{\text{target}}$ with $\mathbf{K}_{\text{target}}^{(l,h)}$ and the dictionary with $\{\mathbf{K}_{D_i}^{(l,h)}\}_{i=1}^k$. The orthogonalized key $\mathbf{r}^{(l,h)}$ is substituted back before the softmax computation, ensuring that the resulting attention weights, and hence the DAAM heatmap, are purged of distractor influence. Because the cross-attention mechanism is linear in the keys prior to softmax, the orthogonality guarantee of Lemma~1 transfers: $\inner{\mathbf{r}^{(l,h)}}{\mathbf{K}_{D_i}^{(l,h)}} = 0$ implies that the attention weight between the target query and distractor keys is minimized.

\section{Experiments}

\subsection{Experimental Setups}
Overall, we test our framework across \textbf{3 different models} (CLIP, SigLIP, Stable Diffusion 2) and \textbf{5 different methods} (GradCAM, LeGrad, CheferCAM, AttentionCAM, DAAM). We evaluate the empirical performance of these methods on the ImageNet-Segmentation dataset \cite{guillaumin2014imagenet}, a curated benchmark containing 4,276 images from the ImageNet validation set equipped with precise pixel-level annotations. Additional experiments are conducted utilizing 1,500 heatmaps, consisting of 5 different methods, their corresponding OSP heatmap, 3 different concept each, and 50 randomly sampled images from the validation sets of Pascal VOC 2012 \cite{everingham2015pascal} and MS COCO \cite{lin2014microsoft}, making it 5 $\times$ 2 $\times$ 3 $\times$ 50 = 1,500 heatmaps. We specifically selected images that contain at least 2 unique objects to effectively test multi-concept disambiguation in clustered scenes.

\subsection{Quantitative Results}
% TODO: Add Quantitative Results.

\subsection{Visual Results}
% TODO: Add Visual qualitative findings and heatmaps.

\subsection{Ablations}
% TODO: Evaluate Ablations of the OSP hyper-parameters if applicable.


\section{User Study}

\subsection{Use Cases}
A key implication of the Hallucination Theorem is that the severity of hallucination is predictive solely from the geometry of the text embedding space, without requiring access to image data or model inference. 
% TODO: Expand on specific predictive detection practical use cases.

\subsection{User study}

To evaluate the practical impact of OSP on interpretability and trust, we conducted a user study inspired by the human-aligned evaluation framework proposed by Kazmierczak et al.~\cite{kazmierczak2024benchmarking}.

\textbf{Experimental Setup.} We compiled a dataset of 50 randomly selected images from the validation sets of the Pascal VOC 2012 \cite{everingham2015pascal} and MS COCO \cite{lin2014microsoft} datasets. Each image was specifically sampled to contain at least 2 unique objects. For each image, users were presented with saliency maps generated by standard attribution methods and our proposed OSP, without knowing the text prompts used to generate them.

\textbf{Study Protocol.} The evaluation followed a structured, two-phase questionnaire for each heatmap:
\begin{enumerate}
    \item \textbf{Object Identification:} Users were asked, \textit{``Based on the heatmap, which class is the model focusing on?''} The available options were:
    \begin{itemize}
        \item Target 1 (existing in the image)
        \item Target 2 (existing in the image)
        \item None of them
    \end{itemize}
    \item \textbf{Confidence Rating:} Before confirming their selection, users reported their confidence level in their answer.
    \item \textbf{Post-Disclosure Assessment:} After the user submitted their answer, the correct target class was revealed. We then asked, \textit{``How well does the heatmap work to understand the correct answer?''} to assess the perceived quality and fidelity of the explanation.
\end{enumerate}

This methodology allows us to quantitatively measure how semantic hallucination in baseline methods misleads users into selecting incorrect objects or ``None of them,'' and how OSP improves both the accuracy of human identification and the users' confidence in the model's explanations.

\section{Conclusion}
We have introduced a unified framework for Linear Semantic Attribution that generalizes across CLIP, SigLIP, and Diffusion-based XAI. By identifying semantic non-orthogonality as the root cause of hallucinations, we proposed OSP as a geometric intervention. We proved that querying with the OMP residual mathematically necessitates the suppression of shared features, enabling precise, hallucination-free explanations in both discriminative and generative foundation models. 

\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
\clearpage\mbox{}Page \thepage\ of the manuscript.
This is the last page of the manuscript.
\par\vfill\par
Now we have reached the maximum size of the ECCV 2016 submission (excluding references).
References should start immediately after the main text, but can continue on p.15 if needed.

\clearpage
\appendix
\section{Excluded Methods}
While our framework theoretically covers a broad range of attribution techniques, we exclude certain methods from our experiments due to specific limitations:

\textbf{TextSpan \cite{textspan2024}:} Although TextSpan aligns with our Linear Semantic Attribution framework, it generates heatmaps for each concept independently before comparison. This post-hoc aggregation is incompatible with our proposed OMP intervention (Section 5), which requires iterative residual computation in the coherent embedding space.

\textbf{Class-Agnostic Methods:} Techniques such as Attention Rollout \cite{rollout2020}, Layer-wise Relevance Propagation (LRP) \cite{lrp2015}, and Raw Attention are inherently class-agnostic. They do not incorporate the text query $\va^{\text{txt}}_c$ into the attribution process, making them unsuitable for addressing text-conditional hallucinations or evaluating semantic leakage.

\clearpage

\begin{thebibliography}{9}

\bibitem{guillaumin2014imagenet}
Guillaumin, M., Kuttel, D., \& Ferrari, V. (2014).
\newblock ImageNet auto-annotation with segmentation propagation.
\newblock \textit{International Journal of Computer Vision}, 110(3), 328--348.

\bibitem{kazmierczak2024benchmarking}
Kazmierczak, R., Azzolin, S., Berthier, E., Hedstr\"om, A., Delhomme, P., Filliat, D., \dots \& Franchi, G. (2024).
\newblock Benchmarking XAI explanations with human-aligned evaluations.
\newblock \textit{arXiv preprint arXiv:2411.02470}.

\bibitem{selvaraju2017grad}
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., \& Batra, D. (2017).
\newblock GradCAM: Visual explanations from deep networks via gradient-based localization.
\newblock In \textit{Proceedings of the IEEE International Conference on Computer Vision (ICCV)} (pp. 618–626).

\bibitem{legrad2025}
Bousselham, W., Boggust, A., Chaybouti, S., Strobelt, H., \& Kuehne, H. (2025).
\newblock LeGrad: An explainability method for vision transformers via feature formation sensitivity.
\newblock In \textit{Proceedings of the IEEE International Conference on Computer Vision (ICCV)}.

\bibitem{textspan2024} 
Gandelsman, Y., Efros, A. A., \& Steinhardt, J. (2024).
\newblock TextSpan: Interpreting CLIP's image representation via text-based decomposition.
\newblock In \textit{Proceedings of the International Conference on Learning Representations (ICLR)}.

\bibitem{rollout2020}
Abnar, S., \& Zuidema, W. (2020).
\newblock Quantifying attention flow in transformers.
\newblock \textit{arXiv preprint arXiv:2005.00928}.

\bibitem{lrp2015}
Bach, S., Binder, A., Montavon, G., Klauschen, F., M\"uller, K. R., \& Samek, W. (2015).
\newblock On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.
\newblock \textit{PLoS one}, 10(7), e0130140.

\bibitem{chefer2020}
Chefer, H., Gur, S., \& Wolf, L. (2020).
\newblock Transformer interpretability beyond attention visualization.
\newblock \textit{arXiv preprint arXiv:2012.09838}.

\bibitem{chefer2021}
Chefer, H., Gur, S., \& Wolf, L. (2021).
\newblock Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers.
\newblock In \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)} (pp. 397--406).

\bibitem{tang-etal-2023-daam}
Tang, R., Liu, L., Pandey, A., Jiang, Z., Yang, G., Kumar, K., Stenetorp, P., Lin, J., \& Ture, F. (2023).
\newblock What the {DAAM}: Interpreting Stable Diffusion Using Cross Attention.
\newblock In \textit{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)} (pp. 5644--5659). Toronto, Canada: Association for Computational Linguistics.



\bibitem{lin2014microsoft}
Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... \& Zitnick, C. L. (2014).
\newblock Microsoft COCO: Common objects in context.
\newblock In \textit{European conference on computer vision} (pp. 740-755). Springer, Cham.

\bibitem{everingham2015pascal}
Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K., Winn, J., \& Zisserman, A. (2015).
\newblock The Pascal visual object classes challenge: A retrospective.
\newblock \textit{International journal of computer vision}, 111(1), 98-136.

\bibitem{li2023evaluating}
Li, Y., Cui, R., Wang, J., Gui, L., \& Wang, Y. (2023).
\newblock Evaluating object hallucination in large vision-language models.
\newblock In \textit{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing} (pp. 292-305).

\bibitem{rohrbach2018object}
Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T., \& Saenko, K. (2018).
\newblock Object hallucination in image captioning.
\newblock In \textit{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing} (pp. 4035-4045).

\bibitem{huh2024platonic}
Huh, M., Cheung, B., Wang, T., Isola, P., Agrawal, P., \& Torralba, A. (2024).
\newblock The platonic representation hypothesis.
\newblock \textit{arXiv preprint arXiv:2405.07987}.

\bibitem{zou2023representation}
Zou, A., Phan, L., Chen, S., Campbell, J., Braun, P., Center, W., ... \& Hendrycks, D. (2023).
\newblock Representation engineering: A top-down approach to AI transparency.
\newblock \textit{arXiv preprint arXiv:2310.01405}.

\bibitem{liang2022mind}
Liang, W., Zhang, Y., Kwon, Y., Yeung, S., \& Zou, J. (2022).
\newblock Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.
\newblock \textit{Advances in Neural Information Processing Systems}, 35, 17612-17625.

\bibitem{bricken2023towards}
Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., ... \& Olah, C. (2023).
\newblock Towards monosemanticity: A dictionary learning architecture for vision models.
\newblock \textit{Distill}, 8(7), e00057.

\bibitem{kazmierczak2025enhancing}
Kazmierczak, R., Azzolin, S., Berthier, E., Frehse, G., \& Franchi, G. (2025).
\newblock Enhancing Concept Localization in CLIP-based Concept Bottleneck Models.
\newblock \textit{arXiv preprint arXiv:2510.07115, Transactions on Machine Learning Research}.

\bibitem{radford2021learning}
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., \& Sutskever, I. (2021).
\newblock Learning transferable visual models from natural language supervision.
\newblock In \textit{Proceedings of the 38th International Conference on Machine Learning (ICML)} (pp. 8748--8763). PMLR.

\bibitem{zhai2023sigmoid}
Zhai, X., Mustafa, B., Kolesnikov, A., \& Beyer, L. (2023).
\newblock Sigmoid loss for language image pre-training.
\newblock In \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)} (pp. 11975--11986).

\bibitem{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., \& Ommer, B. (2022).
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)} (pp. 10684--10695).

\bibitem{esser2024scaling}
Esser, P., Kulal, S., Blattmann, A., Entezari, R., M\"uller, J., Saini, H., Levi, Y., Lorber, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Rombach, R. \& others (2024).
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In \textit{Proceedings of the 41st International Conference on Machine Learning (ICML)}.

\bibitem{chen2024pixart}
Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., \& Li, Z. (2024).
\newblock PixArt-$\alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock In \textit{Proceedings of the International Conference on Learning Representations (ICLR)}.

\bibitem{zhou2024vision}
Zhou, X., Liu, M., Yurtsever, E., Zagar, B. L., Zimmer, W., Cao, Y., \& Knoll, A. C. (2024).
\newblock Vision language models in autonomous driving: A survey and outlook.
\newblock \textit{IEEE Transactions on Intelligent Vehicles}, 9(1), 2024.

\bibitem{kazerouni2023diffusion}
Kazerouni, A., Aghdam, E. K., Heidari, M., Azad, R., Fayyaz, M., Hacihaliloglu, I., \& Merhof, D. (2023).
\newblock Diffusion models in medical imaging: A comprehensive survey.
\newblock \textit{Medical Image Analysis}, 88, 102846.

\bibitem{nasiriany2024pivot}
Nasiriany, S., Xia, F., Yu, W., Xiao, T., Liang, J., Dasgupta, I., Xie, A., Driess, D., Wahid, A., Xu, Z., Vuong, Q., Zhang, T., Lee, T.-W. E., Lee, K.-H., Xu, P., Kirmani, S., Zhu, Y., Ichter, B., Tompson, J., \& Hausman, K. (2024).
\newblock Pivot: Iterative visual prompting elicits actionable knowledge for VLMs.
\newblock In \textit{Proceedings of the 41st International Conference on Machine Learning (ICML)}.

\end{thebibliography}

\end{document}
